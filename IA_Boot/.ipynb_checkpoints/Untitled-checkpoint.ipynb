{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13557daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os\n",
    "from bunch import Bunch\n",
    "import pandas as pd\n",
    "import itertools  \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import defaultdict\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "discours = Bunch()\n",
    "discours.filenames = []\n",
    "discours.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57554192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Path\n",
    "path = \"/home/tom/Documents/Cours/IA/tor_2021_24/discours/\"\n",
    "windowPath = \"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\"\n",
    "# Change the directory\n",
    "os.chdir(windowPath)\n",
    "  \n",
    "\n",
    "#Create my bunch object  \n",
    "def create_bunch(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        discours.data.append(f.read())\n",
    "  \n",
    "  \n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{windowPath}/{file}\"\n",
    "        #Remove weird caract from file title\n",
    "        file = file.replace(\"\\xE7\",\"c\")\n",
    "        file = file.replace(\"\\xE9\",\"e\")\n",
    "        file = file.replace(\"\\xE8\",\"e\")\n",
    "\n",
    "        discours.filenames.append(file)\n",
    "\n",
    "        create_bunch(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(data):\n",
    "\n",
    "    #tokenize with lower + only alphanumerique caract\n",
    "    tokens = [w for w in word_tokenize(data.lower()) if w.isalpha()] \n",
    "\n",
    "    #Remove franch stop words\n",
    "    no_stops = [t for t in tokens if t not in stopwords.words('french')]\n",
    "\n",
    "    #Lemmatize my data\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    bow_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "    return bow_lemmatized\n",
    "\n",
    "bow_lemmatized_speeches = []\n",
    "for speech in discours[\"data\"]:\n",
    "    speech = speech.replace(\"a-\",\"a \")\n",
    "    bow_lemmatized_speeches.append(create_bag_of_words(speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(bow_lemmatized_speeches)\n",
    "\n",
    "corpus = [dictionary.doc2bow(speech) for speech in bow_lemmatized_speeches]\n",
    "\n",
    "bow_doc = sorted(corpus[0], key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "#for word_id, word_count in bow_doc[:5]:\n",
    "    #print(word_id)\n",
    "    #print(dictionary.get(word_id), word_count)\n",
    "\n",
    "total_word_count = defaultdict(int)\n",
    "\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    \n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "#print(dictionary)\n",
    "st = stopwords.words(\"french\")\n",
    "vectorizer = TfidfVectorizer(stop_words=st)\n",
    "X = vectorizer.fit_transform(bow_lemmatized_speeches[0])\n",
    "print(\"x\",X)\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "#print(terms)\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42beb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example = []\n",
    "def create_bunch(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        example.append(f.read())\n",
    "\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Marie_Le_Pen_475.txt\")\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Marie_Le_Pen_684.txt\")\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Marie_Le_Pen_922.txt\")\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Marie_Le_Pen_982.txt\")\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Pierre_Chevènement_504.txt\")\n",
    "create_bunch(\"S:\\Dev\\DataAnalysis\\Data\\IA_Boot\\discours\\\\tous\\Jean-Pierre_Chevènement_818.txt\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "X = vectorizer.transform(example)\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb46a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
